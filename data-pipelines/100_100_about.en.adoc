= Data Pipelines

== Process biodiversity data

Data pipelines provides components to integrate, structure, interpret and transform biodiversity data. Built for extensibility, portability and high performance, data pipelines powers services such as https://www.gbif.org/[GBIF.org].

image::../img/pipeline.png[]

=== Features

Some of the high-level capabilities and objectives of Data pipelines include:

* Support a variety of input formats (https://www.tdwg.org/standards/dwc/[Darwin Core], https://www.tdwg.org/standards/abcd/[ABCD], CSV files, Excel files, Shapefiles etc) with easy opportunity to include new connectors
* Support batch (e.g. a project CSV) and streaming inout (e.g. append-only tracking data)
* Align data to a standardized vocabularies, supporting multilingual data labelling
* Apply quality controls to flag errors, detect outliers and apply statements about the suitability of the data for a variety of uses (also known as fitness for use indicators)
* Enrich data by:
** cross referencing with geospatial gazetteers for political boundaries (e.g. https://gadm.org/[GADM.org], http://vliz.be/vmdcdata/marbound/[EEZ], protected areas) and biogegraphic regions, landuse categorisation and environmental surfaces
** organizating to multiple taxonomic classifications including the https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c[GBIF Backbone taxonomy], http://www.catalogueoflife.org/[Catalogue of Life] and national legislative taxonomies such as https://www.itis.gov/[ITIS]
* Allow consumers to easily understand the data preparation and enrichment process that has been applied (i.e. preserve and document data provenance).
* Provide clear documentation for all data transformations
* Support multiple runtime environments such as https://spark.apache.org/[Apache Spark], https://cloud.google.com/dataflow/[Google Dataflow], https://aws.amazon.com/emr/[Amazon EMR] or local machine
* Ensure pipelines can be deployed in a high throughput environment. https://www.gbif.org/[GBIF.org] target the processing and indexing into Elasticsearch of 1 Billion records in under 12 hours
